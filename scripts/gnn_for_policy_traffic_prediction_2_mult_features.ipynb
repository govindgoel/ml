{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtqdm\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwandb\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import tqdm\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.transforms import LineGraph\n",
    "from shapely.geometry import LineString\n",
    "import gnn_io as gio\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "Here we investigate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameters\n",
    "num_epochs = 40\n",
    "batch_size = 20\n",
    "lr = 0.001\n",
    "project_name = 'multiple_features'\n",
    "train_ratio = 0.8\n",
    "wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data and create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GnnModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = torch_geometric.nn.GCNConv(2, 16)\n",
    "        # self.conv2 = torch_geometric.nn.GATConv(16, 16)\n",
    "        self.conv3 = torch_geometric.nn.GCNConv(16, 1)\n",
    "        self.weight_first_dim = 2.0\n",
    "        # self.conv3 = torch_geometric.nn.GCNConv(16, 1)\n",
    "        # self.gat1 = torch_geometric.nn.GATConv(16, 16)\n",
    "        # self.conv4 = torch_geometric.nn.GCNConv(16, 1)\n",
    "                \n",
    "        # self.convWithPos = torch_geometric.nn.conv.PointNetConv(1, 16, 3)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = x[:, [0, 2]]\n",
    "        # x = x[:, [0, 3]]\n",
    "        x[:, 0] *= self.weight_first_dim\n",
    "        x = self.conv1(x, edge_index)\n",
    "        # x = F.relu(x)\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        # x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        # x = F.relu(x)\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        # x = self.conv3(x, edge_index)\n",
    "        # x = F.relu(x)\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        # x = self.gat1(x, edge_index)\n",
    "        # x = F.relu(x)\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        # x = self.conv4(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_positional_features(dataset):\n",
    "    # Collect all positional features\n",
    "    all_pos_features = []\n",
    "    for data in dataset:\n",
    "        all_pos_features.append(data.pos)\n",
    "\n",
    "    # Stack all positional features into a single tensor\n",
    "    all_pos_features = torch.cat(all_pos_features, dim=0)\n",
    "\n",
    "    # Fit the min-max scaler on the positional features\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(all_pos_features)\n",
    "\n",
    "    # Apply the scaler to each data instance and store as a new feature\n",
    "    for data in dataset:\n",
    "        data.normalized_pos = torch.tensor(scaler.transform(data.pos), dtype=torch.float)\n",
    "    return dataset\n",
    "\n",
    "def normalize_y_values(dataset):\n",
    "    # Collect all y values\n",
    "    all_y_values = []\n",
    "    for data in dataset:\n",
    "        all_y_values.append(data.y)\n",
    "\n",
    "    # Stack all y values into a single tensor\n",
    "    all_y_values = torch.cat(all_y_values, dim=0)\n",
    "\n",
    "    # Fit the min-max scaler on the y values\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(all_y_values)\n",
    "\n",
    "    # Apply the scaler to each data instance and store as a new feature\n",
    "    for data in dataset:\n",
    "        data.normalized_y = torch.tensor(scaler.transform(data.y), dtype=torch.float)  # Keep the 2D shape\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def normalize_dataset(dataset):\n",
    "    # Normalize node features\n",
    "    dataset = normalize_data(dataset)\n",
    "    # Normalize positional features (if any)\n",
    "    dataset = normalize_positional_features(dataset)\n",
    "    # Normalize y values\n",
    "    dataset = normalize_y_values(dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def replace_invalid_values(tensor):\n",
    "    finite_mask = torch.isfinite(tensor)\n",
    "    if finite_mask.any():\n",
    "        max_finite_value = tensor[finite_mask].max()\n",
    "    else:\n",
    "        max_finite_value = torch.tensor(0.0)\n",
    "    tensor[~finite_mask] = max_finite_value\n",
    "    tensor[torch.isnan(tensor)] = max_finite_value\n",
    "    return tensor\n",
    "\n",
    "def normalize_data(dataset):\n",
    "    shape_of_x = dataset[0].x.shape[1]\n",
    "    if shape_of_x == 1:\n",
    "        # Collect all node features\n",
    "        all_node_features = []\n",
    "        for data in dataset:\n",
    "            all_node_features.append(data.x)\n",
    "\n",
    "        # Stack all node features into a single tensor\n",
    "        all_node_features = torch.cat(all_node_features, dim=0)\n",
    "\n",
    "        # Fit the min-max scaler on the node features\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(all_node_features)\n",
    "\n",
    "        # Apply the scaler to each data instance and store as a new feature\n",
    "        for data in dataset:\n",
    "            data.normalized_x = torch.tensor(scaler.transform(data.x), dtype=torch.float)\n",
    "            \n",
    "    else:\n",
    "        normalized_x = []\n",
    "        for i in range(shape_of_x):\n",
    "            all_node_features_this_dimension = []\n",
    "            # Concatenate all x attributes across the dataset, keeping the feature dimension\n",
    "            for data in dataset:\n",
    "                all_node_features_this_dimension.append(data.x[:,i])\n",
    "                \n",
    "            all_node_features_this_dimension = torch.cat(all_node_features_this_dimension, dim=0).reshape(-1, 1)\n",
    "        \n",
    "            # Replace infinity values with the maximum finite value in the tensor\n",
    "            finite_mask = torch.isfinite(all_node_features_this_dimension)\n",
    "            max_finite_value = all_node_features_this_dimension[finite_mask].max()\n",
    "            all_node_features_this_dimension[~finite_mask] = max_finite_value\n",
    "\n",
    "            # Check and handle NaN values\n",
    "            nan_mask = torch.isnan(all_node_features_this_dimension)\n",
    "            all_node_features_this_dimension[nan_mask] = max_finite_value\n",
    "\n",
    "            # Convert tensor to numpy array for scikit-learn\n",
    "            all_node_features_np = all_node_features_this_dimension.numpy()\n",
    "            \n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(all_node_features_np)\n",
    "            for data in dataset:\n",
    "                data_x_dim_replaced = replace_invalid_values(data.x[:, i].reshape(-1, 1))\n",
    "                normalized_x_this_dimension = torch.tensor(scaler.transform(data_x_dim_replaced.numpy()), dtype=torch.float)\n",
    "                \n",
    "                if i == 0:\n",
    "                    data.normalized_x = normalized_x_this_dimension\n",
    "                else:\n",
    "                    data.normalized_x = torch.cat((data.normalized_x, normalized_x_this_dimension), dim=1)\n",
    "            \n",
    "            normalized_x.append(normalized_x_this_dimension)\n",
    "        normalized_x = torch.cat(normalized_x, dim=1)\n",
    "        data.normalized_x = normalized_x  \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of dictionaries\n",
    "data_dict_list = torch.load('../data/dataset_1pm_0-1382_with_more_infos.pt')\n",
    "\n",
    "# Reconstruct the Data objects\n",
    "datalist = [Data(x=d['x'], edge_index=d['edge_index'], pos=d['pos'], y=d['y']) for d in data_dict_list]\n",
    "\n",
    "# # Apply normalization to your dataset\n",
    "dataset_normalized = normalize_dataset(datalist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate MSE - baseline error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of y: 0.030937498435378075\n",
      "Standard Deviation of y: 0.074600949883461\n",
      "Baseline error is: 0.005565311759710312\n"
     ]
    }
   ],
   "source": [
    "y_values_normalized = np.concatenate([data.normalized_y for data in dataset_normalized])\n",
    "\n",
    "# Compute the mean and standard deviation\n",
    "mean_y_normalized = np.mean(y_values_normalized)\n",
    "std_y_normalized = np.std(y_values_normalized)\n",
    "\n",
    "print(f\"Mean of y: {mean_y_normalized}\")\n",
    "print(f\"Standard Deviation of y: {std_y_normalized}\")\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "y_values_normalized_tensor = torch.tensor(y_values_normalized, dtype=torch.float32)\n",
    "mean_y_normalized_tensor = torch.tensor(mean_y_normalized, dtype=torch.float32)\n",
    "\n",
    "# Create the target tensor with the same shape as y_values_normalized_tensor\n",
    "target_tensor = mean_y_normalized_tensor * torch.ones_like(y_values_normalized_tensor)\n",
    "\n",
    "# Instantiate the MSELoss function\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Compute the MSE \n",
    "mse = mse_loss(y_values_normalized_tensor, target_tensor)\n",
    "\n",
    "# Print the MSE value\n",
    "print(\"Baseline error is: \" + str(mse.item()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load model and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1rkvirrh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf8ee3dba5f43dba34f3c216eb49421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">icy-valley-8</strong> at: <a href='https://wandb.ai/tum-traffic-engineering/multiple_features/runs/1rkvirrh' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/multiple_features/runs/1rkvirrh</a><br/> View project at: <a href='https://wandb.ai/tum-traffic-engineering/multiple_features' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/multiple_features</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240705_111620-1rkvirrh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1rkvirrh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e79512dfca5433aa6a05380b6c65319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011167903711116702, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/elenanatterer/Development/MATSim/eqasim-java/ile_de_france/src/main/python/gnn/wandb/run-20240705_111705-wxoh5bqs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tum-traffic-engineering/multiple_features/runs/wxoh5bqs' target=\"_blank\">fragrant-surf-9</a></strong> to <a href='https://wandb.ai/tum-traffic-engineering/multiple_features' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tum-traffic-engineering/multiple_features' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/multiple_features</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tum-traffic-engineering/multiple_features/runs/wxoh5bqs' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/multiple_features/runs/wxoh5bqs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "wandb.init(\n",
    "        project=project_name,\n",
    "        config={\n",
    "            \"epochs\": num_epochs,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"lr\": lr,\n",
    "            'early_stopping_patience': 10,\n",
    "            # \"dropout\": 0.15,\n",
    "            })\n",
    "config = wandb.config\n",
    "\n",
    "model = GnnModel().to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fct = torch.nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset length: 1382\n",
      "Training subset length: 1100\n",
      "Total dataset length: 1382\n",
      "Validation subset length: 260\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "train_dl = gio.create_dataloader(dataset=dataset_normalized, is_train=True, batch_size=config.batch_size, train_ratio=train_ratio)\n",
    "valid_dl = gio.create_dataloader(dataset=dataset_normalized, is_train=False, batch_size=config.batch_size, train_ratio=train_ratio)\n",
    "n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)\n",
    "print(n_steps_per_epoch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the model\n",
    "\n",
    "We first find a good model for one batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m early_stopping \u001b[39m=\u001b[39m gio\u001b[39m.\u001b[39mEarlyStopping(patience\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mepochs):\n\u001b[1;32m      4\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gio' is not defined"
     ]
    }
   ],
   "source": [
    "early_stopping = gio.EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    model.train()\n",
    "    data = next(iter(train_dl))\n",
    "    # for idx in range(len(train_dl)):\n",
    "    # for idx, data in enumerate(train_dl):\n",
    "    input_node_features, targets = data.normalized_x.to(device), data.normalized_y.to(device)\n",
    "    predicted = model(data)\n",
    "    train_loss = loss_fct(predicted, targets)\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    # wandb.log({\"train_loss\": train_loss.item(), \"epoch\": epoch, \"step\": idx})\n",
    "        # print(f\"epoch: {epoch}, step: {idx}, loss: {train_loss.item()}\")\n",
    "        \n",
    "    val_loss = gio.validate_model(model, valid_dl, loss_fct, device)\n",
    "    wandb.log({\"val_loss\": val_loss})\n",
    "    print(f\"epoch: {epoch}, val_loss: {val_loss}\")\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered. Stopping training.\")\n",
    "        break\n",
    "    \n",
    "wandb.summary[\"val_loss\"] = val_loss\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paris_Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
